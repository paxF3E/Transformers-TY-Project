{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Defaulting to user installation because normal site-packages is not writeable\n",
                        "Requirement already satisfied: transformers in /home/paxx/.local/lib/python3.10/site-packages (4.28.0.dev0)\n",
                        "Requirement already satisfied: datasets in /home/paxx/.local/lib/python3.10/site-packages (2.10.2.dev0)\n",
                        "Requirement already satisfied: sentencepiece in /home/paxx/.local/lib/python3.10/site-packages (0.1.97)\n",
                        "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (0.12.1)\n",
                        "Requirement already satisfied: numpy>=1.17 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (1.23.1)\n",
                        "Requirement already satisfied: tqdm>=4.27 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
                        "Requirement already satisfied: packaging>=20.0 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (21.3)\n",
                        "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (0.13.1)\n",
                        "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.25.1)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (2022.9.13)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
                        "Requirement already satisfied: filelock in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (3.8.0)\n",
                        "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/paxx/.local/lib/python3.10/site-packages (from datasets) (2022.8.2)\n",
                        "Requirement already satisfied: aiohttp in /home/paxx/.local/lib/python3.10/site-packages (from datasets) (3.8.3)\n",
                        "Requirement already satisfied: multiprocess in /home/paxx/.local/lib/python3.10/site-packages (from datasets) (0.70.13)\n",
                        "Requirement already satisfied: pyarrow>=6.0.0 in /home/paxx/.local/lib/python3.10/site-packages (from datasets) (9.0.0)\n",
                        "Requirement already satisfied: pandas in /home/paxx/.local/lib/python3.10/site-packages (from datasets) (1.4.3)\n",
                        "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/paxx/.local/lib/python3.10/site-packages (from datasets) (0.3.5.1)\n",
                        "Requirement already satisfied: responses<0.19 in /home/paxx/.local/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
                        "Requirement already satisfied: xxhash in /home/paxx/.local/lib/python3.10/site-packages (from datasets) (3.1.0)\n",
                        "Requirement already satisfied: frozenlist>=1.1.1 in /home/paxx/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
                        "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/paxx/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\n",
                        "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/paxx/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
                        "Requirement already satisfied: yarl<2.0,>=1.0 in /home/paxx/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.1)\n",
                        "Requirement already satisfied: multidict<7.0,>=4.5 in /home/paxx/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.2)\n",
                        "Requirement already satisfied: attrs>=17.3.0 in /home/paxx/.local/lib/python3.10/site-packages (from aiohttp->datasets) (22.1.0)\n",
                        "Requirement already satisfied: aiosignal>=1.1.2 in /home/paxx/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.2.0)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/paxx/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
                        "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
                        "Requirement already satisfied: urllib3>=1.25.10 in /home/paxx/.local/lib/python3.10/site-packages (from responses<0.19->datasets) (1.26.15)\n",
                        "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.1 in /home/paxx/.local/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
                        "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
                        "Requirement already satisfied: idna>=2.0 in /home/paxx/.local/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (2.10)\n",
                        "Defaulting to user installation because normal site-packages is not writeable\n",
                        "Requirement already satisfied: transformers in /home/paxx/.local/lib/python3.10/site-packages (4.28.0.dev0)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (2022.9.13)\n",
                        "Requirement already satisfied: tqdm>=4.27 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
                        "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (0.13.1)\n",
                        "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.25.1)\n",
                        "Requirement already satisfied: packaging>=20.0 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (21.3)\n",
                        "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (0.12.1)\n",
                        "Requirement already satisfied: numpy>=1.17 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (1.23.1)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
                        "Requirement already satisfied: filelock in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (3.8.0)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/paxx/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
                        "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n"
                    ]
                }
            ],
            "source": [
                "!pip install transformers datasets sentencepiece\n",
                "!pip install -q pytorch-lightning wandb\n",
                "!pip install transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
                        "  warnings.warn(\n",
                        "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
                        "  warnings.warn(\n",
                        "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
                        "  warnings.warn(\n",
                        "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
                        "  warnings.warn(\n",
                        "2023-04-09 02:17:46.970065: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
                        "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
                        "2023-04-09 02:17:49.039361: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
                        "2023-04-09 02:17:49.039586: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
                        "2023-04-09 02:17:49.039598: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "from pytorch_lightning import Trainer\n",
                "from pytorch_lightning.loggers import WandbLogger\n",
                "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
                "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
                "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
                "from transformers import Trainer, TrainingArguments\n",
                "import pandas as pd\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_dataset(file_path, tokenizer, block_size = 128):\n",
                "    dataset = TextDataset(\n",
                "        tokenizer = tokenizer,\n",
                "        file_path = file_path,\n",
                "        block_size = block_size,\n",
                "    )\n",
                "    return dataset\n",
                "\n",
                "\n",
                "def load_data_collator(tokenizer, mlm = False):\n",
                "    data_collator = DataCollatorForLanguageModeling(\n",
                "        tokenizer=tokenizer, \n",
                "        mlm=mlm,\n",
                "    )\n",
                "    return data_collator"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_func(train_file_path,\n",
                "          model_name,\n",
                "          output_dir,\n",
                "          overwrite_output_dir,\n",
                "          per_device_train_batch_size,\n",
                "          num_train_epochs,\n",
                "          save_steps):\n",
                "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
                "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
                "  data_collator = load_data_collator(tokenizer)\n",
                "\n",
                "  tokenizer.save_pretrained(output_dir)\n",
                "      \n",
                "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
                "\n",
                "  model.save_pretrained(output_dir)\n",
                "\n",
                "  training_args = TrainingArguments(\n",
                "          output_dir=output_dir,\n",
                "          overwrite_output_dir=overwrite_output_dir,\n",
                "          per_device_train_batch_size=per_device_train_batch_size,\n",
                "          num_train_epochs=num_train_epochs,\n",
                "          save_steps=save_steps,\n",
                "          gpus=torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
                "      )\n",
                "\n",
                "  trainer = Trainer(\n",
                "          model=model,\n",
                "          args=training_args,\n",
                "          data_collator=data_collator,\n",
                "          train_dataset=train_dataset,\n",
                "          gpus=torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
                "  )\n",
                "      \n",
                "  trainer.train()\n",
                "  trainer.save_model()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "#  to set parameters \n",
                "train_file_path = \"Datasets_PART2/SPoC/train/split/spoc-train-train.tsv\"\n",
                "model_name = 'gpt2'\n",
                "output_dir = 'result/'\n",
                "overwrite_output_dir = True\n",
                "per_device_train_batch_size = 8\n",
                "num_train_epochs = 5.0\n",
                "save_steps = 100"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/paxx/.local/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
                        "  warnings.warn(\n",
                        "/home/paxx/.local/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
                        "  warnings.warn(\n",
                        "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpaxx\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "24a66401473844a899e51043fcfc6218",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016670091099998292, max=1.0…"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "wandb version 0.14.2 is available!  To upgrade, please run:\n",
                            " $ pip install wandb --upgrade"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Tracking run with wandb version 0.14.0"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Run data is saved locally in <code>/media/data2/Transformers/Transformers-TY-Project/wandb/run-20230409_022404-n5ai0u80</code>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Syncing run <strong><a href='https://wandb.ai/paxx/huggingface/runs/n5ai0u80' target=\"_blank\">rare-aardvark-5</a></strong> to <a href='https://wandb.ai/paxx/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View project at <a href='https://wandb.ai/paxx/huggingface' target=\"_blank\">https://wandb.ai/paxx/huggingface</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View run at <a href='https://wandb.ai/paxx/huggingface/runs/n5ai0u80' target=\"_blank\">https://wandb.ai/paxx/huggingface/runs/n5ai0u80</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "082b81cdd39c4558bb222ff7d87f0467",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/34955 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "train_func(\n",
                "    train_file_path=train_file_path,\n",
                "    model_name=model_name,\n",
                "    output_dir=output_dir,\n",
                "    overwrite_output_dir=overwrite_output_dir,\n",
                "    per_device_train_batch_size=per_device_train_batch_size,\n",
                "    num_train_epochs=num_train_epochs,\n",
                "    save_steps=save_steps\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_pseudocode(input_code, model_path):\n",
                "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
                "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
                "    input_tokens = tokenizer.encode(input_code, return_tensors='pt')\n",
                "\n",
                "    output_tokens = model.generate(\n",
                "        input_ids=input_tokens,\n",
                "        max_length=128,\n",
                "        temperature=0.7,\n",
                "        num_return_sequences=1,\n",
                "        pad_token_id=tokenizer.eos_token_id\n",
                "    )\n",
                "\n",
                "    output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
                "    return output_text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "input_code = \"int add(int a, int b){ return a + b; }\"\n",
                "output = generate_pseudocode(input_code=input_code, model_path=output_dir)\n",
                "pseudo_code = output.split(\"/t\")[0]\n",
                "print(pseudo_code)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.6"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
