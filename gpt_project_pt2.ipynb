{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Defaulting to user installation because normal site-packages is not writeable\n",
                        "Requirement already satisfied: transformers in /home/paxx/.local/lib/python3.10/site-packages (4.28.0.dev0)\n",
                        "Requirement already satisfied: datasets in /home/paxx/.local/lib/python3.10/site-packages (2.10.2.dev0)\n",
                        "Requirement already satisfied: sentencepiece in /home/paxx/.local/lib/python3.10/site-packages (0.1.97)\n",
                        "Requirement already satisfied: tqdm>=4.27 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
                        "Requirement already satisfied: numpy>=1.17 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (1.23.1)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (2022.9.13)\n",
                        "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (0.13.1)\n",
                        "Requirement already satisfied: packaging>=20.0 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (21.3)\n",
                        "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.25.1)\n",
                        "Requirement already satisfied: filelock in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (3.8.0)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
                        "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (0.12.1)\n",
                        "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/paxx/.local/lib/python3.10/site-packages (from datasets) (0.3.5.1)\n",
                        "Requirement already satisfied: pandas in /home/paxx/.local/lib/python3.10/site-packages (from datasets) (1.4.3)\n",
                        "Requirement already satisfied: xxhash in /home/paxx/.local/lib/python3.10/site-packages (from datasets) (3.1.0)\n",
                        "Requirement already satisfied: multiprocess in /home/paxx/.local/lib/python3.10/site-packages (from datasets) (0.70.13)\n",
                        "Requirement already satisfied: responses<0.19 in /home/paxx/.local/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
                        "Requirement already satisfied: aiohttp in /home/paxx/.local/lib/python3.10/site-packages (from datasets) (3.8.3)\n",
                        "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/paxx/.local/lib/python3.10/site-packages (from datasets) (2022.8.2)\n",
                        "Requirement already satisfied: pyarrow>=6.0.0 in /home/paxx/.local/lib/python3.10/site-packages (from datasets) (9.0.0)\n",
                        "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/paxx/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
                        "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/paxx/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\n",
                        "Requirement already satisfied: frozenlist>=1.1.1 in /home/paxx/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
                        "Requirement already satisfied: aiosignal>=1.1.2 in /home/paxx/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.2.0)\n",
                        "Requirement already satisfied: yarl<2.0,>=1.0 in /home/paxx/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.1)\n",
                        "Requirement already satisfied: attrs>=17.3.0 in /home/paxx/.local/lib/python3.10/site-packages (from aiohttp->datasets) (22.1.0)\n",
                        "Requirement already satisfied: multidict<7.0,>=4.5 in /home/paxx/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.2)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/paxx/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
                        "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
                        "Requirement already satisfied: urllib3>=1.25.10 in /home/paxx/.local/lib/python3.10/site-packages (from responses<0.19->datasets) (1.26.15)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.1 in /home/paxx/.local/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
                        "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
                        "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
                        "Requirement already satisfied: idna>=2.0 in /home/paxx/.local/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (2.10)\n"
                    ]
                }
            ],
            "source": [
                "!pip install transformers datasets sentencepiece\n",
                "!pip install -q pytorch-lightning wandb"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "training_sample = pd.read_table(\"Datasets_PART2/SPoC/train/split/spoc-train-train.tsv\", usecols=[\"text\",\"code\"])\n",
                "test_sample = pd.read_table(\"Datasets_PART2/SPoC/train/split/spoc-train-test.tsv\", usecols=[\"text\",\"code\"])\n",
                "eval_sample = pd.read_table(\"Datasets_PART2/SPoC/train/split/spoc-train-eval.tsv\", usecols=[\"text\",\"code\"])\n",
                "\n",
                "training_sample = training_sample.dropna()\n",
                "test_sample = test_sample.dropna()\n",
                "eval_sample = eval_sample.dropna()\n",
                "\n",
                "training_sample = training_sample.reset_index(drop=True)\n",
                "test_sample = test_sample.reset_index(drop=True)\n",
                "eval_sample = eval_sample.reset_index(drop=True)\n",
                "\n",
                "training_sample = training_sample.iloc[:100000]\n",
                "test_sample = test_sample.iloc[:15000]\n",
                "eval_sample = eval_sample.iloc[:15000]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/paxx/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
                        "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
                        "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
                        "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
                        "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
                        "  warnings.warn(\n"
                    ]
                }
            ],
            "source": [
                "from transformers import T5Tokenizer\n",
                "\n",
                "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
                "max_input_length = 256\n",
                "max_target_length = 128\n",
                "\n",
                "def preprocess_samples(dataset):\n",
                "    text = dataset[\"text\"]\n",
                "    code = dataset[\"code\"]\n",
                "\n",
                "    model_inputs = tokenizer(text, max_length = max_input_length, padding=\"max_length\", truncation=True)\n",
                "    labels = tokenizer(code, max_length=max_target_length, padding=\"max_length\", truncation=True).input_ids\n",
                "\n",
                "    labels_with_ignore_index = []\n",
                "    for labels_sample in labels:\n",
                "      labels_sample = [label if label != 0 else -100 for label in labels_sample]\n",
                "      labels_with_ignore_index.append(labels_sample)\n",
                "\n",
                "    model_inputs[\"labels\"] = labels_with_ignore_index\n",
                "\n",
                "    return model_inputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "DatasetDict({\n",
                            "    train: Dataset({\n",
                            "        features: ['text', 'code'],\n",
                            "        num_rows: 100000\n",
                            "    })\n",
                            "    test: Dataset({\n",
                            "        features: ['text', 'code'],\n",
                            "        num_rows: 15000\n",
                            "    })\n",
                            "    eval: Dataset({\n",
                            "        features: ['text', 'code'],\n",
                            "        num_rows: 15000\n",
                            "    })\n",
                            "})"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from datasets import Dataset, load_dataset, DatasetDict\n",
                "train = Dataset.from_dict(training_sample)\n",
                "test = Dataset.from_dict(test_sample)\n",
                "eval = Dataset.from_dict(eval_sample)\n",
                "\n",
                "dataset = DatasetDict({\"train\" : train, \"test\": test,\"eval\": eval})\n",
                "dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e0154c6b4f4c40f48d40a71064201e6d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3363e7dc03e34ac79cbe5e74a81ffd95",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "06dc581f625743d9bd59deb3a9e6e2ac",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "DatasetDict({\n",
                            "    train: Dataset({\n",
                            "        features: ['text', 'code', 'input_ids', 'attention_mask', 'labels'],\n",
                            "        num_rows: 100000\n",
                            "    })\n",
                            "    test: Dataset({\n",
                            "        features: ['text', 'code', 'input_ids', 'attention_mask', 'labels'],\n",
                            "        num_rows: 15000\n",
                            "    })\n",
                            "    eval: Dataset({\n",
                            "        features: ['text', 'code', 'input_ids', 'attention_mask', 'labels'],\n",
                            "        num_rows: 15000\n",
                            "    })\n",
                            "})"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "dataset = dataset.map(preprocess_samples, batched=True)\n",
                "dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.utils.data import DataLoader\n",
                "\n",
                "dataset.set_format(type=\"torch\", columns=['input_ids','attention_mask','labels'])\n",
                "train_dataloader = DataLoader(dataset['train'], batch_size=8)\n",
                "valid_dataloader = DataLoader(dataset['eval'], batch_size=4)\n",
                "test_dataloader = DataLoader(dataset['test'], batch_size=4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
                    ]
                }
            ],
            "source": [
                "batch = next(iter(train_dataloader))\n",
                "print(batch.keys())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2023-03-20 22:28:41.321550: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
                        "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
                        "2023-03-20 22:28:42.771587: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
                        "2023-03-20 22:28:42.771723: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
                        "2023-03-20 22:28:42.771736: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "'create string s</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "batch = next(iter(train_dataloader))\n",
                "print(batch.keys())\n",
                "tokenizer.decode(batch['input_ids'][0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'string s;</s>'"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "labels = batch['labels'][0]\n",
                "tokenizer.decode([label for label in labels if label != -100])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
                        "  warnings.warn(\n",
                        "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
                        "  warnings.warn(\n",
                        "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
                        "  warnings.warn(\n",
                        "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
                        "  warnings.warn(\n"
                    ]
                }
            ],
            "source": [
                "from pytorch_lightning import Trainer\n",
                "from pytorch_lightning.loggers import WandbLogger\n",
                "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Defaulting to user installation because normal site-packages is not writeable\n",
                        "Requirement already satisfied: transformers in /home/paxx/.local/lib/python3.10/site-packages (4.28.0.dev0)\n",
                        "Requirement already satisfied: numpy>=1.17 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (1.23.1)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (2022.9.13)\n",
                        "Requirement already satisfied: tqdm>=4.27 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
                        "Requirement already satisfied: filelock in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (3.8.0)\n",
                        "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (0.13.1)\n",
                        "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.25.1)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
                        "Requirement already satisfied: packaging>=20.0 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (21.3)\n",
                        "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/paxx/.local/lib/python3.10/site-packages (from transformers) (0.12.1)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/paxx/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
                        "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n"
                    ]
                }
            ],
            "source": [
                "!pip install transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
                "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
                "from transformers import Trainer, TrainingArguments"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_dataset(file_path, tokenizer, block_size = 128):\n",
                "    dataset = TextDataset(\n",
                "        tokenizer = tokenizer,\n",
                "        file_path = file_path,\n",
                "        block_size = block_size,\n",
                "    )\n",
                "    return dataset\n",
                "\n",
                "\n",
                "def load_data_collator(tokenizer, mlm = False):\n",
                "    data_collator = DataCollatorForLanguageModeling(\n",
                "        tokenizer=tokenizer, \n",
                "        mlm=mlm,\n",
                "    )\n",
                "    return data_collator"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_func(train_file_path,model_name,\n",
                "          output_dir,\n",
                "          overwrite_output_dir,\n",
                "          per_device_train_batch_size,\n",
                "          num_train_epochs,\n",
                "          save_steps):\n",
                "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
                "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
                "  data_collator = load_data_collator(tokenizer)\n",
                "\n",
                "  tokenizer.save_pretrained(output_dir)\n",
                "      \n",
                "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
                "\n",
                "  model.save_pretrained(output_dir)\n",
                "\n",
                "  training_args = TrainingArguments(\n",
                "          output_dir=output_dir,\n",
                "          overwrite_output_dir=overwrite_output_dir,\n",
                "          per_device_train_batch_size=per_device_train_batch_size,\n",
                "          num_train_epochs=num_train_epochs,\n",
                "      )\n",
                "\n",
                "  trainer = Trainer(\n",
                "          model=model,\n",
                "          args=training_args,\n",
                "          data_collator=data_collator,\n",
                "          train_dataset=train_dataset,\n",
                "  )\n",
                "      \n",
                "  trainer.train()\n",
                "  trainer.save_model()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "# you need to set parameters \n",
                "train_file_path = \"Datasets_PART2/SPoC/train/split/spoc-train-train.tsv\"\n",
                "model_name = 'gpt2'\n",
                "output_dir = 'result/'\n",
                "overwrite_output_dir = True\n",
                "per_device_train_batch_size = 8\n",
                "num_train_epochs = 5.0\n",
                "save_steps = 100"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/paxx/.local/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
                        "  warnings.warn(\n",
                        "/home/paxx/.local/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
                        "  warnings.warn(\n",
                        "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpaxx\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f0be2392ac1c4ce081c4b48757ed289a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666930653300369, max=1.0)…"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Tracking run with wandb version 0.14.0"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Run data is saved locally in <code>/media/data2/Transformers/wandb/run-20230320_222902-yb89roab</code>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Syncing run <strong><a href='https://wandb.ai/paxx/huggingface/runs/yb89roab' target=\"_blank\">major-shadow-4</a></strong> to <a href='https://wandb.ai/paxx/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View project at <a href='https://wandb.ai/paxx/huggingface' target=\"_blank\">https://wandb.ai/paxx/huggingface</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View run at <a href='https://wandb.ai/paxx/huggingface/runs/yb89roab' target=\"_blank\">https://wandb.ai/paxx/huggingface/runs/yb89roab</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "6545868f281444e0b7e39e8245da5c80",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/34955 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'loss': 1.2984, 'learning_rate': 4.928479473608926e-05, 'epoch': 0.07}\n",
                        "{'loss': 0.9341, 'learning_rate': 4.8569589472178516e-05, 'epoch': 0.14}\n",
                        "{'loss': 0.886, 'learning_rate': 4.785438420826778e-05, 'epoch': 0.21}\n",
                        "{'loss': 0.8507, 'learning_rate': 4.713917894435703e-05, 'epoch': 0.29}\n",
                        "{'loss': 0.828, 'learning_rate': 4.6423973680446294e-05, 'epoch': 0.36}\n",
                        "{'loss': 0.8168, 'learning_rate': 4.5708768416535544e-05, 'epoch': 0.43}\n",
                        "{'loss': 0.8007, 'learning_rate': 4.499356315262481e-05, 'epoch': 0.5}\n",
                        "{'loss': 0.7932, 'learning_rate': 4.4278357888714065e-05, 'epoch': 0.57}\n",
                        "{'loss': 0.7779, 'learning_rate': 4.356315262480332e-05, 'epoch': 0.64}\n",
                        "{'loss': 0.7652, 'learning_rate': 4.284794736089258e-05, 'epoch': 0.72}\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[1;32m/media/data2/Transformers/gpt_project_pt2.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_func(\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     train_file_path\u001b[39m=\u001b[39;49mtrain_file_path,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model_name\u001b[39m=\u001b[39;49mmodel_name,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     output_dir\u001b[39m=\u001b[39;49moutput_dir,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     overwrite_output_dir\u001b[39m=\u001b[39;49moverwrite_output_dir,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39;49mper_device_train_batch_size,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39;49mnum_train_epochs,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     save_steps\u001b[39m=\u001b[39;49msave_steps\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n",
                        "\u001b[1;32m/media/data2/Transformers/gpt_project_pt2.ipynb Cell 17\u001b[0m in \u001b[0;36mtrain_func\u001b[0;34m(train_file_path, model_name, output_dir, overwrite_output_dir, per_device_train_batch_size, num_train_epochs, save_steps)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         output_dir\u001b[39m=\u001b[39moutput_dir,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         overwrite_output_dir\u001b[39m=\u001b[39moverwrite_output_dir,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         per_device_train_batch_size\u001b[39m=\u001b[39mper_device_train_batch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         num_train_epochs\u001b[39m=\u001b[39mnum_train_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         data_collator\u001b[39m=\u001b[39mdata_collator,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         train_dataset\u001b[39m=\u001b[39mtrain_dataset,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/data2/Transformers/gpt_project_pt2.ipynb#X22sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m trainer\u001b[39m.\u001b[39msave_model()\n",
                        "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1630\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1632\u001b[0m )\n\u001b[0;32m-> 1633\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1634\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1635\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1636\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1637\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1638\u001b[0m )\n",
                        "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1902\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1900\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1901\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1902\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1904\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1905\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1906\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1907\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1908\u001b[0m ):\n\u001b[1;32m   1909\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1910\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
                        "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2662\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2660\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[1;32m   2661\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2662\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m   2664\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
                        "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
                        "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "train_func(\n",
                "    train_file_path=train_file_path,\n",
                "    model_name=model_name,\n",
                "    output_dir=output_dir,\n",
                "    overwrite_output_dir=overwrite_output_dir,\n",
                "    per_device_train_batch_size=per_device_train_batch_size,\n",
                "    num_train_epochs=num_train_epochs,\n",
                "    save_steps=save_steps\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.6"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
